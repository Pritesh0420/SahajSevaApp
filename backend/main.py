import os
import uuid
import json
import re
from typing import List, Optional

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv

from google import genai
from google.genai import types
from gtts import gTTS
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException
from googletrans import Translator
import uvicorn

import pdfplumber
import pytesseract
from PIL import Image

# ---------------- ENV + GOOGLE GEMINI ----------------
load_dotenv()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    client = genai.Client(api_key=GOOGLE_API_KEY)
    MODEL_NAME = 'gemini-1.5-flash'
else:
    client = None
    MODEL_NAME = None

# When true, the API will return a best-effort, non-LLM result if the LLM is
# unavailable (missing key, quota exhausted, network issues, etc.).
ALLOW_ANALYZE_WITHOUT_LLM = os.getenv("ALLOW_ANALYZE_WITHOUT_LLM", "true").strip().lower() in (
    "1",
    "true",
    "yes",
    "y",
)

UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# Session storage for conversation state (in-memory, use Redis/DB for production)
conversation_sessions = {}

# ---------------- APP ----------------
app = FastAPI(title="Sahaj Seva AI Backend", version="5.0.0")


STATE_DISPLAY = {
    # States
    "andhra pradesh": {"en": "Andhra Pradesh", "hi": "‡§Ü‡§Ç‡§ß‡•ç‡§∞ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂"},
    "arunachal pradesh": {"en": "Arunachal Pradesh", "hi": "‡§Ö‡§∞‡•Å‡§£‡§æ‡§ö‡§≤ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂"},
    "assam": {"en": "Assam", "hi": "‡§Ö‡§∏‡§Æ"},
    "bihar": {"en": "Bihar", "hi": "‡§¨‡§ø‡§π‡§æ‡§∞"},
    "chhattisgarh": {"en": "Chhattisgarh", "hi": "‡§õ‡§§‡•ç‡§§‡•Ä‡§∏‡§ó‡§¢‡§º"},
    "goa": {"en": "Goa", "hi": "‡§ó‡•ã‡§µ‡§æ"},
    "gujarat": {"en": "Gujarat", "hi": "‡§ó‡•Å‡§ú‡§∞‡§æ‡§§"},
    "haryana": {"en": "Haryana", "hi": "‡§π‡§∞‡§ø‡§Ø‡§æ‡§£‡§æ"},
    "himachal pradesh": {"en": "Himachal Pradesh", "hi": "‡§π‡§ø‡§Æ‡§æ‡§ö‡§≤ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂"},
    "jharkhand": {"en": "Jharkhand", "hi": "‡§ù‡§æ‡§∞‡§ñ‡§Ç‡§°"},
    "karnataka": {"en": "Karnataka", "hi": "‡§ï‡§∞‡•ç‡§®‡§æ‡§ü‡§ï"},
    "kerala": {"en": "Kerala", "hi": "‡§ï‡•á‡§∞‡§≤"},
    "madhya pradesh": {"en": "Madhya Pradesh", "hi": "‡§Æ‡§ß‡•ç‡§Ø ‡§™‡•ç‡§∞‡§¶‡•á‡§∂"},
    "maharashtra": {"en": "Maharashtra", "hi": "‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞"},
    "manipur": {"en": "Manipur", "hi": "‡§Æ‡§£‡§ø‡§™‡•Å‡§∞"},
    "meghalaya": {"en": "Meghalaya", "hi": "‡§Æ‡•á‡§ò‡§æ‡§≤‡§Ø"},
    "mizoram": {"en": "Mizoram", "hi": "‡§Æ‡§ø‡§ú‡•ã‡§∞‡§Æ"},
    "nagaland": {"en": "Nagaland", "hi": "‡§®‡§æ‡§ó‡§æ‡§≤‡•à‡§Ç‡§°"},
    "odisha": {"en": "Odisha", "hi": "‡§ì‡§°‡§ø‡§∂‡§æ"},
    "punjab": {"en": "Punjab", "hi": "‡§™‡§Ç‡§ú‡§æ‡§¨"},
    "rajasthan": {"en": "Rajasthan", "hi": "‡§∞‡§æ‡§ú‡§∏‡•ç‡§•‡§æ‡§®"},
    "sikkim": {"en": "Sikkim", "hi": "‡§∏‡§ø‡§ï‡•ç‡§ï‡§ø‡§Æ"},
    "tamil nadu": {"en": "Tamil Nadu", "hi": "‡§§‡§Æ‡§ø‡§≤‡§®‡§æ‡§°‡•Å"},
    "telangana": {"en": "Telangana", "hi": "‡§§‡•á‡§≤‡§Ç‡§ó‡§æ‡§®‡§æ"},
    "tripura": {"en": "Tripura", "hi": "‡§§‡•ç‡§∞‡§ø‡§™‡•Å‡§∞‡§æ"},
    "uttar pradesh": {"en": "Uttar Pradesh", "hi": "‡§â‡§§‡•ç‡§§‡§∞ ‡§™‡•ç‡§∞‡§¶‡•á‡§∂"},
    "uttarakhand": {"en": "Uttarakhand", "hi": "‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ñ‡§Ç‡§°"},
    "west bengal": {"en": "West Bengal", "hi": "‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§¨‡§Ç‡§ó‡§æ‡§≤"},

    # Union Territories
    "andaman and nicobar islands": {"en": "Andaman and Nicobar Islands", "hi": "‡§Ö‡§Ç‡§°‡§Æ‡§æ‡§® ‡§î‡§∞ ‡§®‡§ø‡§ï‡•ã‡§¨‡§æ‡§∞ ‡§¶‡•ç‡§µ‡•Ä‡§™‡§∏‡§Æ‡•Ç‡§π"},
    "chandigarh": {"en": "Chandigarh", "hi": "‡§ö‡§Ç‡§°‡•Ä‡§ó‡§¢‡§º"},
    "dadra and nagar haveli and daman and diu": {"en": "Dadra and Nagar Haveli and Daman and Diu", "hi": "‡§¶‡§æ‡§¶‡§∞‡§æ ‡§î‡§∞ ‡§®‡§ó‡§∞ ‡§π‡§µ‡•á‡§≤‡•Ä ‡§î‡§∞ ‡§¶‡§Æ‡§® ‡§î‡§∞ ‡§¶‡•Ä‡§µ"},
    "delhi": {"en": "Delhi", "hi": "‡§¶‡§ø‡§≤‡•ç‡§≤‡•Ä"},
    "jammu and kashmir": {"en": "Jammu and Kashmir", "hi": "‡§ú‡§Æ‡•ç‡§Æ‡•Ç ‡§î‡§∞ ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞"},
    "ladakh": {"en": "Ladakh", "hi": "‡§≤‡§¶‡•ç‡§¶‡§æ‡§ñ"},
    "lakshadweep": {"en": "Lakshadweep", "hi": "‡§≤‡§ï‡•ç‡§∑‡§¶‡•ç‡§µ‡•Ä‡§™"},
    "puducherry": {"en": "Puducherry", "hi": "‡§™‡•Å‡§°‡•Å‡§ö‡•á‡§∞‡•Ä"},
}

UT_KEYS = {
    "andaman and nicobar islands",
    "chandigarh",
    "dadra and nagar haveli and daman and diu",
    "delhi",
    "jammu and kashmir",
    "ladakh",
    "lakshadweep",
    "puducherry",
}


@app.get("/api/meta/states")
def get_states_meta():
    items = []
    for key, labels in STATE_DISPLAY.items():
        items.append(
            {
                "key": key,
                "en": labels.get("en", ""),
                "hi": labels.get("hi", ""),
                "type": "ut" if key in UT_KEYS else "state",
            }
        )
    items.sort(key=lambda x: x.get("en", ""))
    return {"states": items}


def _scheme_key(name: str) -> str:
    raw = _norm_text_lower(name)
    out = []
    prev_dash = False
    for ch in raw:
        is_alnum = ("a" <= ch <= "z") or ("0" <= ch <= "9")
        if is_alnum:
            out.append(ch)
            prev_dash = False
        else:
            if not prev_dash:
                out.append("-")
                prev_dash = True
    key = "".join(out).strip("-")
    return key or "scheme"


@app.get("/api/meta/schemes")
def get_schemes_meta():
    items = []
    seen = set()
    for i, scheme in enumerate(SCHEMES):
        en = (scheme.get("name") or "").strip()
        hi = (scheme.get("name_hi") or "").strip()
        base = _scheme_key(en or hi or str(i))
        key = base
        # Ensure unique keys even if names collide.
        n = 2
        while key in seen:
            key = f"{base}-{n}"
            n += 1
        seen.add(key)
        items.append({"key": key, "en": en, "hi": hi})
    items.sort(key=lambda x: x.get("en", ""))
    return {"schemes": items}

# Map user-entered state names (English/Hindi/aliases) to our canonical keys.
STATE_ALIASES = {
    "orissa": "odisha",
    "uttaranchal": "uttarakhand",
    "pondicherry": "puducherry",
    "nct of delhi": "delhi",
}


def _norm_text(value: str) -> str:
    return unicodedata.normalize("NFKC", str(value or "")).strip()


def _norm_text_lower(value: str) -> str:
    return _norm_text(value).lower()
for _key, _labels in STATE_DISPLAY.items():
    # Canonical key
    STATE_ALIASES[_norm_text_lower(_key)] = _key
    # English display (lowercase)
    _en = _norm_text_lower(_labels.get("en") or "")
    if _en:
        STATE_ALIASES[_en] = _key
    # Hindi display (exact + trimmed)
    _hi = _norm_text(_labels.get("hi") or "")
    if _hi:
        STATE_ALIASES[_hi] = _key

# Common Hindi spellings / variants (add as needed)
_extra_aliases = {
    "‡§Æ‡§ß‡•ç‡§Ø ‡§™‡•ç‡§∞‡§¶‡•á‡§∂": "madhya pradesh",
    "‡§Ü‡§Ç‡§ß‡•ç‡§∞‡§™‡•ç‡§∞‡§¶‡•á‡§∂": "andhra pradesh",
    "‡§â‡§§‡•ç‡§§‡§∞‡§™‡•ç‡§∞‡§¶‡•á‡§∂": "uttar pradesh",
    "‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡§¨‡§Ç‡§ó‡§æ‡§≤": "west bengal",
    "‡§§‡§Æ‡§ø‡§≤ ‡§®‡§æ‡§°‡•Å": "tamil nadu",
    "‡§ú‡§Æ‡•ç‡§Æ‡•Ç ‡§ï‡§∂‡•ç‡§Æ‡•Ä‡§∞": "jammu and kashmir",
}
for _k, _v in _extra_aliases.items():
    STATE_ALIASES[_norm_text(_k)] = _v
    STATE_ALIASES[_norm_text(_k).replace(" ", "")] = _v


def _normalize_state_key(state_value: str) -> str:
    if not state_value:
        return ""
    raw = _norm_text(state_value)
    if not raw:
        return ""
    # Try direct match first (Hindi + normalized)
    if raw in STATE_ALIASES:
        return STATE_ALIASES[raw]

    lowered = raw.lower()
    if lowered in STATE_ALIASES:
        return STATE_ALIASES[lowered]

    # Also try without spaces (common user input)
    compact = raw.replace(" ", "")
    if compact in STATE_ALIASES:
        return STATE_ALIASES[compact]
    compact_lower = lowered.replace(" ", "")
    if compact_lower in STATE_ALIASES:
        return STATE_ALIASES[compact_lower]

    return lowered


def _normalize_gender_value(gender_value: str) -> str:
    g = (gender_value or "").strip()
    if not g:
        return ""
    gl = g.lower()
    mapping = {
        "male": "male",
        "m": "male",
        "man": "male",
        "‡§™‡•Å‡§∞‡•Å‡§∑": "male",
        "female": "female",
        "f": "female",
        "woman": "female",
        "‡§Æ‡§π‡§ø‡§≤‡§æ": "female",
        "other": "other",
        "‡§Ö‡§®‡•ç‡§Ø": "other",
    }
    return mapping.get(g, mapping.get(gl, gl))


def _normalize_occupation_value(occupation_value: str) -> str:
    o = (occupation_value or "").strip()
    if not o:
        return ""
    ol = o.lower()
    mapping = {
        "farmer": "farmer",
        "‡§ï‡§ø‡§∏‡§æ‡§®": "farmer",
        "student": "student",
        "‡§õ‡§æ‡§§‡•ç‡§∞": "student",
        "‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•Ä": "student",
        "labour": "labour",
        "labor": "labour",
        "worker": "labour",
        "‡§Æ‡§ú‡§¶‡•Ç‡§∞": "labour",
        "teacher": "teacher",
        "‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï": "teacher",
        "business": "business",
        "‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞": "business",
        "‡§¶‡•Å‡§ï‡§æ‡§®‡§¶‡§æ‡§∞": "business",
    }
    return mapping.get(o, mapping.get(ol, ol))


OCCUPATION_DISPLAY = {
    "farmer": {"en": "Farmer", "hi": "‡§ï‡§ø‡§∏‡§æ‡§®"},
    "student": {"en": "Student", "hi": "‡§õ‡§æ‡§§‡•ç‡§∞"},
    "labour": {"en": "Worker", "hi": "‡§Æ‡§ú‡§¶‡•Ç‡§∞"},
    "teacher": {"en": "Teacher", "hi": "‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï"},
    "business": {"en": "Business", "hi": "‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞"},
    "other": {"en": "Other", "hi": "‡§Ö‡§®‡•ç‡§Ø"},
}


def _profile_for_explanation(*, user: dict, match_user: dict, language: str) -> dict:
    p = dict(user)

    occ_key = (match_user.get("occupation") or "").strip().lower()
    if occ_key in OCCUPATION_DISPLAY:
        p["occupation"] = OCCUPATION_DISPLAY[occ_key].get(language) or OCCUPATION_DISPLAY[occ_key].get("en")

    state_key = (match_user.get("state") or "").strip().lower()
    if state_key in STATE_DISPLAY:
        p["state"] = STATE_DISPLAY[state_key].get(language) or STATE_DISPLAY[state_key].get("en")

    gender_key = (match_user.get("gender") or "").strip().lower()
    if language == "hi":
        gender_hi = {"male": "‡§™‡•Å‡§∞‡•Å‡§∑", "female": "‡§Æ‡§π‡§ø‡§≤‡§æ", "other": "‡§Ö‡§®‡•ç‡§Ø"}.get(gender_key)
        if gender_hi:
            p["gender"] = gender_hi
    else:
        gender_en = {"male": "Male", "female": "Female", "other": "Other"}.get(gender_key)
        if gender_en:
            p["gender"] = gender_en

    return p


def _state_portal_result(*, state_key: str, language: str) -> dict:
    state_key = _normalize_state_key(state_key)
    label = STATE_DISPLAY.get(state_key, {}).get(language) or STATE_DISPLAY.get(state_key, {}).get("en") or state_key

    if language == "hi":
        return {
            "name": f"{label} ‡§ï‡•á ‡§∞‡§æ‡§ú‡•ç‡§Ø-‡§∏‡•ç‡§§‡§∞‡•Ä‡§Ø ‡§Ø‡•ã‡§ú‡§®‡§æ‡§è‡§Å (‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï ‡§™‡•ã‡§∞‡•ç‡§ü‡§≤)",
            "benefits": "‡§Ö‡§™‡§®‡•á ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡•Ä ‡§Ø‡•ã‡§ú‡§®‡§æ‡§è‡§Å ‡§¶‡•á‡§ñ‡•á‡§Ç, ‡§™‡§æ‡§§‡•ç‡§∞‡§§‡§æ ‡§ú‡§æ‡§Ç‡§ö‡•á‡§Ç, ‡§î‡§∞ ‡§Ü‡§µ‡•á‡§¶‡§®/‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§™‡§æ‡§è‡§Ç‡•§",
            "why": "‡§Ø‡§π ‡§Ü‡§™‡§ï‡•á ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï/‡§∞‡§æ‡§ú‡•ç‡§Ø-‡§∏‡•ç‡§§‡§∞‡•Ä‡§Ø ‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§¢‡•Ç‡§Å‡§¢‡§®‡•á ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§§‡§∞‡•Ä‡§ï‡§æ ‡§π‡•à‡•§",
        }

    return {
        "name": f"{label} State Schemes (Official Portal)",
        "benefits": "Browse your state‚Äôs schemes, check eligibility, and get application/status information.",
        "why": "This is the safest way to find verified, state-specific schemes for your location.",
    }

def is_eligible(scheme, user):
    elig = scheme.get("eligibility", {})

    def _to_list(value):
        if value is None:
            return []
        if isinstance(value, list):
            return value
        return [value]

    # Occupation: string or list
    if "occupation" in elig and elig["occupation"] != "any":
        if not user.get("occupation"):
            return False
        allowed = [str(x).strip().lower() for x in _to_list(elig.get("occupation")) if str(x).strip()]
        if allowed and str(user["occupation"]).strip().lower() not in allowed:
            return False

    # Gender: string or list
    if "gender" in elig and elig["gender"] != "any":
        if not user.get("gender"):
            return False
        allowed = [str(x).strip().lower() for x in _to_list(elig.get("gender")) if str(x).strip()]
        if allowed and str(user["gender"]).strip().lower() not in allowed:
            return False

    # Age bounds
    if "age_min" in elig:
        if user.get("age") is None:
            return False
        if int(user["age"]) < int(elig["age_min"]):
            return False
    if "age_max" in elig:
        if user.get("age") is None:
            return False
        if int(user["age"]) > int(elig["age_max"]):
            return False

    # Income bounds
    if "income_min" in elig:
        if user.get("income") is None:
            return False
        if float(user["income"]) < float(elig["income_min"]):
            return False
    if "income_max" in elig:
        if user.get("income") is None:
            return False
        if float(user["income"]) > float(elig["income_max"]):
            return False

    # State: string or list
    if "state" in elig and elig["state"] != "any":
        if not user.get("state"):
            return False
        allowed = [str(x).strip().lower() for x in _to_list(elig.get("state")) if str(x).strip()]
        if allowed and str(user["state"]).strip().lower() not in allowed:
            return False

    return True


ai_client = get_ai_client()


@app.post("/api/profile/extract", response_model=ExtractProfileResponse)
async def extract_profile(payload: ExtractProfileRequest):
    extracted = await ai_client.extract_profile(text=payload.text, language=payload.language)
    return ExtractProfileResponse(
        age=extracted.age,
        gender=extracted.gender,
        occupation=extracted.occupation,
        income=extracted.income,
        state=extracted.state,
    )

@app.post("/api/scheme-finder", response_model=SchemeFinderResponse)
async def scheme_finder(payload: SchemeFinderRequest, request: Request):
    user = payload.model_dump()

    # Normalize user-entered values for reliable matching.
    match_user = dict(user)
    match_user["gender"] = _normalize_gender_value(match_user.get("gender") or "")
    match_user["occupation"] = _normalize_occupation_value(match_user.get("occupation") or "")
    match_user["state"] = _normalize_state_key(match_user.get("state") or "")

    explain_profile = _profile_for_explanation(user=user, match_user=match_user, language=payload.language)

    eligible = []
    for scheme in SCHEMES:
        if is_eligible(scheme, match_user):
            scheme_name = scheme.get("name_hi") if payload.language == "hi" and scheme.get("name_hi") else scheme.get("name", "")
            scheme_benefits = (
                scheme.get("benefits_hi")
                if payload.language == "hi" and scheme.get("benefits_hi")
                else scheme.get("benefits", "")
            )
            scheme_rule_explanation = (
                scheme.get("explanation_hi")
                if payload.language == "hi" and scheme.get("explanation_hi")
                else scheme.get("explanation", "")
            )
            why = await ai_client.explain_scheme(
                scheme_name=scheme_name,
                scheme_benefits=scheme_benefits,
                scheme_rule_explanation=scheme_rule_explanation,
                profile=explain_profile,
                language=payload.language,
            )

            # Defensive de-duplication: benefits are displayed separately in the UI.
            if scheme_benefits and why:
                if payload.language == "hi":
                    why = why.replace(f"‡§≤‡§æ‡§≠: {scheme_benefits}", "").replace("  ", " ").strip(" -:‡•§\n\t ")
                else:
                    why = why.replace(f"Benefit: {scheme_benefits}", "").replace("  ", " ").strip(" -:.\n\t ")
            eligible.append(
                {
                    "name": scheme_name,
                    "benefits": scheme_benefits,
                    "why": why,
                    "portal_url": scheme.get("portal_url", ""),
                }
            )

    # Always add a state-specific "official portal" suggestion if we recognize the state.
    state_key = match_user.get("state") or ""
    if state_key and state_key in STATE_DISPLAY:
        eligible.append(_state_portal_result(state_key=state_key, language=payload.language))

    return {"schemes": eligible}

# Configure CORS

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount uploads directory
app.mount("/uploads", StaticFiles(directory=UPLOAD_DIR), name="uploads")

# ---------------- MODELS ----------------
class UserProfile(BaseModel):
    age: int
    occupation: str
    income: float
    location: Optional[str] = None


class FormField(BaseModel):
    field_name: str
    field_type: str
    required: bool
    description: str
    example: str


class FormAnalysis(BaseModel):
    form_id: str
    form_name: str
    purpose: str
    eligibility: str
    fields: List[FormField]
    warnings: List[str]


class UserResponse(BaseModel):
    session_id: str
    response: str


class FieldQuestion(BaseModel):
    field_name: str
    question: str
    voice_url: Optional[str]
    example: str


# ---------------- ROUTES ----------------
@app.get("/")
def root():
    return {"status": "Sahaj Seva AI Backend Running üöÄ"}


@app.get("/health")
def health():
    return {"ok": True}


# ---------------- HELPERS ----------------
def extract_text_from_file(path: str, ext: str) -> str:
    text = ""

    if ext == "pdf":
        with pdfplumber.open(path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""

    elif ext in ["png", "jpg", "jpeg"]:
        image = Image.open(path)
        text = pytesseract.image_to_string(image)

    else:
        raise HTTPException(400, "Only PDF, PNG, JPG, JPEG files allowed")

    if not text.strip():
        raise HTTPException(400, "‚ùå Could not extract any text from file")

    return text


def _basic_field_guess_from_text(extracted_text: str, max_fields: int = 25) -> List[FormField]:
    fields: List[FormField] = []
    seen = set()

    # Look for common form field patterns - improved to capture longer field names
    patterns = [
        r"([A-Za-z][A-Za-z0-9 ,/\-]{3,80})\s*[:\-‚Äì_]+\s*",  # Name: _____ or Address Details:
        r"\d+\.\s*([A-Za-z][A-Za-z0-9 ,/\-]{3,80})(?:\s*[:\-‚Äì_]|$)",  # 1. Name or 1. Full Address
        r"([A-Za-z][A-Za-z0-9 ,/\-]{3,80})\s*\(",  # Name(
    ]
    
    # Photo field keywords
    photo_keywords = ['photo', 'photograph', 'image', 'picture', 'pic']

    for raw_line in extracted_text.splitlines():
        line = re.sub(r"\s+", " ", raw_line).strip()
        if not line or len(line) < 3 or len(line) > 200:
            continue

        for pattern in patterns:
            matches = re.findall(pattern, line, re.IGNORECASE)
            for match in matches:
                candidate_name = match.strip()
                
                # Skip common words that aren't field names
                skip_words = {'page', 'form', 'application', 'government', 'date', 'the', 'and', 'for', 'of', 'to', 
                             'please', 'provide', 'enter', 'fill', 'write', 'details', 'information'}
                
                # Clean up trailing punctuation and extra words
                candidate_name = re.sub(r'[:\-‚Äì_]+$', '', candidate_name).strip()
                
                # Skip if too short or in skip words
                if len(candidate_name) < 3 or candidate_name.lower() in skip_words:
                    continue
                
                # Skip if already seen
                if candidate_name.lower() in seen:
                    continue

                seen.add(candidate_name.lower())
                
                # Detect if this is a photo/image field
                field_type = "photo" if any(keyword in candidate_name.lower() for keyword in photo_keywords) else "text"

                fields.append(
                    FormField(
                        field_name=candidate_name,
                        field_type=field_type,
                        required=False,
                        description=f"Please provide your {candidate_name.lower()}",
                        example="",
                    )
                )

                if len(fields) >= max_fields:
                    return fields

    return fields if fields else [
        FormField(
            field_name="Information",
            field_type="text",
            required=False,
            description="Please provide required information",
            example=""
        )
    ]


def _fallback_form_analysis(extracted_text: str, original_filename: str) -> FormAnalysis:
    guessed_fields = _basic_field_guess_from_text(extracted_text)
    
    # Try to extract form name from text
    form_name = "Application Form"
    first_lines = extracted_text.split('\n')[:5]
    for line in first_lines:
        if len(line.strip()) > 5 and len(line.strip()) < 100:
            form_name = line.strip()
            break
    
    # Basic purpose extraction
    purpose = "This form requires you to fill in the following information"
    if 'application' in extracted_text.lower():
        purpose = "This is an application form that requires your personal and relevant information"
    elif 'registration' in extracted_text.lower():
        purpose = "This is a registration form"
    
    return FormAnalysis(
        form_id=str(uuid.uuid4()),
        form_name=form_name,
        purpose=purpose,
        eligibility="Please check the form for eligibility criteria",
        fields=guessed_fields,
        warnings=[
            "Please read the form carefully before filling",
            "Ensure all required information is accurate",
        ],
    )


def _translate_text(text: str, target_lang: str) -> str:
    """Translate text to target language."""
    if not text or target_lang == "en":
        return text
    
    try:
        translator = Translator()
        result = translator.translate(text, dest=target_lang)
        return result.text
    except Exception as e:
        print(f"Translation error: {e}")
        return text  # Return original if translation fails


def _create_intro_text(form_name: str, purpose: str, fields: List[dict], lang: str) -> str:
    """Create a comprehensive introduction that explains form details and all fields."""
    
    # Translate form name and purpose to selected language
    translated_form_name = _translate_text(form_name, lang)
    translated_purpose = _translate_text(purpose, lang)
    
    # Build field list description
    field_names = [f['field_name'] for f in fields] if fields else []
    
    if lang == "hi":
        intro = f"‡§Ø‡§π ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º {translated_form_name} ‡§π‡•à‡•§ {translated_purpose}\n\n"
        intro += "‡§á‡§∏ ‡§´‡•â‡§∞‡•ç‡§Æ ‡§Æ‡•á‡§Ç ‡§®‡§ø‡§Æ‡•ç‡§®‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§π‡•à:\n"
        for i, field in enumerate(fields[:10], 1):  # Limit to 10 for voice clarity
            translated_field_name = _translate_text(field['field_name'], lang)
            translated_description = _translate_text(field.get('description', ''), lang)
            intro += f"{i}. {translated_field_name} - {translated_description}\n"
        if len(fields) > 10:
            intro += f"‡§î‡§∞ {len(fields) - 10} ‡§Ö‡§ß‡§ø‡§ï ‡§´‡•Ä‡§≤‡•ç‡§°‡•ç‡§∏‡•§\n"
        intro += "\n‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§á‡§∏ ‡§´‡•â‡§∞‡•ç‡§Æ ‡§ï‡•ã ‡§≠‡§∞‡§®‡§æ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡•á‡§Ç‡§ó‡•á?"
    elif lang == "mr":
        intro = f"‡§π‡§æ ‡§¶‡§∏‡•ç‡§§‡§ê‡§µ‡§ú {translated_form_name} ‡§Ü‡§π‡•á‡•§ {translated_purpose}\n\n"
        intro += "‡§Ø‡§æ ‡§´‡•â‡§∞‡•ç‡§Æ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ñ‡§æ‡§≤‡•Ä‡§≤ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§Ü‡§π‡•á:\n"
        for i, field in enumerate(fields[:10], 1):
            translated_field_name = _translate_text(field['field_name'], lang)
            translated_description = _translate_text(field.get('description', ''), lang)
            intro += f"{i}. {translated_field_name} - {translated_description}\n"
        if len(fields) > 10:
            intro += f"‡§Ü‡§£‡§ø {len(fields) - 10} ‡§Ö‡§ß‡§ø‡§ï ‡§´‡•Ä‡§≤‡•ç‡§°‡•§\n"
        intro += "\n‡§§‡•Å‡§Æ‡•ç‡§π‡•Ä ‡§π‡§æ ‡§´‡•â‡§∞‡•ç‡§Æ ‡§≠‡§∞‡§£‡•á ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•Ç ‡§á‡§ö‡•ç‡§õ‡§ø‡§§‡§æ ‡§ï‡§æ?"
    else:  # Default English
        intro = f"This document is {translated_form_name}. {translated_purpose}\n\n"
        intro += "This form requires the following information:\n"
        for i, field in enumerate(fields[:10], 1):
            intro += f"{i}. {field['field_name']} - {field.get('description', '')}\n"
        if len(fields) > 10:
            intro += f"and {len(fields) - 10} more fields.\n"
        intro += "\nWould you like to start filling this form?"
    
    return intro


def _create_voice_note(explanation_text: str, target_lang: str = None) -> tuple[Optional[str], str]:
    text = (explanation_text or "").strip() or "Form analysis is ready."

    # Use target language if provided, otherwise auto-detect
    if target_lang:
        lang = target_lang
    else:
        try:
            lang = detect(text)
        except LangDetectException:
            lang = "en"

    audio_filename = f"{uuid.uuid4()}.mp3"
    audio_path = os.path.join(UPLOAD_DIR, audio_filename)

    try:
        gTTS(text=text, lang=lang).save(audio_path)
    except Exception:
        # Fallback to English if the language is not supported
        lang = "en"
        try:
            gTTS(text=text, lang=lang).save(audio_path)
        except Exception:
            return None, lang

    return f"/uploads/{audio_filename}", lang


# üîπ FORM ANALYSIS + VOICE NOTE
@app.post("/api/analyze-form")
async def analyze_form(
    file: UploadFile = File(...),
    language: str = Form(default="en")  # Language code: 'en', 'hi', 'mr', 'ta', etc.
):
    try:
        ext = file.filename.split(".")[-1].lower()
        filename = f"{uuid.uuid4()}.{ext}"
        path = os.path.join(UPLOAD_DIR, filename)

        with open(path, "wb") as f:
            f.write(await file.read())

        extracted_text = extract_text_from_file(path, ext)

        if client is None:
            if not ALLOW_ANALYZE_WITHOUT_LLM:
                raise HTTPException(
                    status_code=503,
                    detail={
                        "code": "llm_not_configured",
                        "message": "GOOGLE_API_KEY is not configured on the server.",
                    },
                )

            result_json = _fallback_form_analysis(extracted_text, file.filename).model_dump()
            
            # Create introductory voice note in selected language
            intro_text = _create_intro_text(
                result_json.get("form_name", "Form"), 
                result_json.get("purpose", ""), 
                result_json.get("fields", []),
                language
            )
            voice_note_url, lang = _create_voice_note(intro_text, language)
            
            return {
                "form_analysis": result_json,
                "voice_note_url": voice_note_url,
                "language_detected": lang,
                "fallback": True,
            }

        prompt = f"""
You are analyzing an Indian government application form.

Return JSON only:
{{
  "form_id": "...",
  "form_name": "...",
  "purpose": "...",
  "eligibility": "...",
  "fields": [
    {{
      "field_name": "...",
      "field_type": "text/number/date",
      "required": true,
      "description": "...",
      "example": "..."
    }}
  ],
  "warnings": ["..."]
}}

Use very simple language for senior citizens.

FORM TEXT:
\"\"\"{extracted_text}\"\"\"
"""

        try:
            full_prompt = "You only return valid JSON. No markdown. No explanation.\n\n" + prompt
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=full_prompt,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                ),
            )
            result_text = response.text
        except Exception as e:
            error_str = str(e).lower()
            # Handle quota/rate limiting
            if "quota" in error_str or "rate" in error_str or "429" in error_str:
                if not ALLOW_ANALYZE_WITHOUT_LLM:
                    raise HTTPException(
                        status_code=429,
                        detail={
                            "code": "gemini_rate_limited",
                            "message": "LLM request was rate-limited or quota was exceeded.",
                            "provider_error": str(e),
                        },
                    )
                result_json = _fallback_form_analysis(extracted_text, file.filename).model_dump()
                intro_text = _create_intro_text(
                    result_json.get("form_name", "Form"), 
                    result_json.get("purpose", ""), 
                    result_json.get("fields", []),
                    language
                )
                voice_note_url, lang = _create_voice_note(intro_text, language)
                return {"form_analysis": result_json, "voice_note_url": voice_note_url, "language_detected": lang, "fallback": True}
            
            # Handle authentication errors
            elif "api key" in error_str or "invalid" in error_str or "auth" in error_str:
                if not ALLOW_ANALYZE_WITHOUT_LLM:
                    raise HTTPException(
                        status_code=401,
                        detail={
                            "code": "gemini_auth_failed",
                            "message": "LLM authentication failed. Check GOOGLE_API_KEY.",
                            "provider_error": str(e),
                        },
                    )
                result_json = _fallback_form_analysis(extracted_text, file.filename).model_dump()
                intro_text = _create_intro_text(
                    result_json.get("form_name", "Form"), 
                    result_json.get("purpose", ""), 
                    result_json.get("fields", []),
                    language
                )
                voice_note_url, lang = _create_voice_note(intro_text, language)
                return {"form_analysis": result_json, "voice_note_url": voice_note_url, "language_detected": lang, "fallback": True}
            
            # Handle connection errors
            else:
                if not ALLOW_ANALYZE_WITHOUT_LLM:
                    raise HTTPException(
                        status_code=503,
                        detail={
                            "code": "gemini_error",
                            "message": "Could not reach the LLM provider or other error occurred.",
                            "provider_error": str(e),
                        },
                    )
                result_json = _fallback_form_analysis(extracted_text, file.filename).model_dump()
                intro_text = _create_intro_text(
                    result_json.get("form_name", "Form"), 
                    result_json.get("purpose", ""), 
                    result_json.get("fields", []),
                    language
                )
                voice_note_url, lang = _create_voice_note(intro_text, language)
                return {"form_analysis": result_json, "voice_note_url": voice_note_url, "language_detected": lang, "fallback": True}
        # Clean up response text (remove markdown code blocks if present)
        result_text = result_text.strip()
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]
        result_text = result_text.strip()
        
        try:
            result_json = json.loads(result_text)
        except Exception as e:
            if ALLOW_ANALYZE_WITHOUT_LLM:
                result_json = _fallback_form_analysis(extracted_text, file.filename).model_dump()
                intro_text = _create_intro_text(
                    result_json.get("form_name", "Form"), 
                    result_json.get("purpose", ""), 
                    result_json.get("fields", []),
                    language
                )
                voice_note_url, lang = _create_voice_note(intro_text, language)
                return {
                    "form_analysis": result_json,
                    "voice_note_url": voice_note_url,
                    "language_detected": lang,
                    "fallback": True,
                    "warning": "LLM returned invalid JSON; using fallback analysis instead.",
                }

            raise HTTPException(
                status_code=502,
                detail={
                    "code": "invalid_llm_json",
                    "message": "LLM response was not valid JSON.",
                    "provider_error": str(e),
                },
            )

        # üîä Create Voice Note with introduction and all field details
        form_name = result_json.get("form_name", "Form")
        purpose = result_json.get("purpose", "")
        fields = result_json.get("fields", [])
        
        intro_text = _create_intro_text(form_name, purpose, fields, language)
        voice_note_url, lang = _create_voice_note(intro_text, language)

        # Create session for conversation
        session_id = str(uuid.uuid4())
        conversation_sessions[session_id] = {
            "form_analysis": result_json,
            "language": language,
            "current_field_index": 0,
            "field_responses": {},
            "form_language": "hi" if any(ord(c) > 127 for c in extracted_text[:100]) else "en",
            "original_file_path": path,
        }
        
        # Clean up the uploaded file after processing (keep only voice notes)
        try:
            if os.path.exists(path):
                os.remove(path)
        except Exception as e:
            print(f"Could not delete file {path}: {e}")

        return {
            "session_id": session_id,
            "form_analysis": result_json,
            "voice_note_url": voice_note_url,
            "language_detected": lang
        }

    except Exception as e:
        print("üî• ERROR:", e)
        raise HTTPException(status_code=500, detail=str(e))


# üîπ START FORM FILLING CONVERSATION
@app.post("/api/start-filling")
async def start_filling(session_id: str = Form(...)):
    """User confirmed they want to start filling the form"""
    if session_id not in conversation_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = conversation_sessions[session_id]
    fields = session["form_analysis"].get("fields", [])
    
    if not fields:
        return {
            "completed": True,
            "message": "No fields to fill"
        }
    
    # Ask first field
    return await _get_next_field_question(session_id)


# üîπ GET NEXT FIELD QUESTION
async def _get_next_field_question(session_id: str):
    session = conversation_sessions[session_id]
    fields = session["form_analysis"].get("fields", [])
    current_index = session["current_field_index"]
    language = session["language"]
    
    if current_index >= len(fields):
        # All fields completed
        return {
            "completed": True,
            "message": "All fields completed! Generating filled form..."
        }
    
    field = fields[current_index]
    
    # Translate field name and description to selected language
    translated_field_name = _translate_text(field['field_name'], language)
    translated_description = _translate_text(field['description'], language)
    
    # Create question in selected language (NO VOICE NOTE - already played comprehensive intro)
    question_texts = {
        "en": f"Please provide: {translated_field_name}. {translated_description}",
        "hi": f"‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡•á‡§Ç: {translated_field_name}‡•§ {translated_description}",
        "mr": f"‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§æ: {translated_field_name}‡•§ {translated_description}",
        "ta": f"‡Æ§‡ÆØ‡Æµ‡ØÅ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡ØÅ ‡Æµ‡Æ¥‡Æô‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç: {translated_field_name}‡•§ {translated_description}",
        "te": f"‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø: {translated_field_name}‡•§ {translated_description}",
        "bn": f"‡¶Ö‡¶®‡ßÅ‡¶ó‡ßç‡¶∞‡¶π ‡¶ï‡¶∞‡ßá ‡¶™‡ßç‡¶∞‡¶¶‡¶æ‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®: {translated_field_name}‡•§ {translated_description}",
        "gu": f"‡™ï‡´É‡™™‡™æ ‡™ï‡™∞‡´Ä‡™®‡´á ‡™™‡´ç‡™∞‡™¶‡™æ‡™® ‡™ï‡™∞‡´ã: {translated_field_name}‡•§ {translated_description}",
        "kn": f"‡≤¶‡≤Ø‡≤µ‡≤ø‡≤ü‡≥ç‡≤ü‡≥Å ‡≤í‡≤¶‡≤ó‡≤ø‡≤∏‡≤ø: {translated_field_name}‡•§ {translated_description}",
        "ml": f"‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥®‡µΩ‡¥ï‡µÅ‡¥ï: {translated_field_name}‡•§ {translated_description}",
        "pa": f"‡®ï‡®ø‡®∞‡®™‡®æ ‡®ï‡®∞‡®ï‡©á ‡®™‡©ç‡®∞‡®¶‡®æ‡®® ‡®ï‡®∞‡©ã: {translated_field_name}‡•§ {translated_description}",
    }
    
    question_text = question_texts.get(language, question_texts["en"])
    
    # Add example if available
    if field.get('example'):
        example_texts = {
            "en": f"Example: {field['example']}",
            "hi": f"‡§â‡§¶‡§æ‡§π‡§∞‡§£: {field['example']}",
            "mr": f"‡§â‡§¶‡§æ‡§π‡§∞‡§£: {field['example']}",
            "ta": f"‡Æâ‡Æ§‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Øç: {field['example']}",
            "te": f"‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£: {field['example']}",
            "bn": f"‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£: {field['example']}",
            "gu": f"‡™â‡™¶‡™æ‡™π‡™∞‡™£: {field['example']}",
            "kn": f"‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü: {field['example']}",
            "ml": f"‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Ç: {field['example']}",
            "pa": f"‡®â‡®¶‡®æ‡®π‡®∞‡®£: {field['example']}",
        }
        question_text += " " + example_texts.get(language, example_texts["en"])
    
    # Generate voice note for this question
    voice_url, _ = _create_voice_note(question_text, language)
    
    return {
        "completed": False,
        "field_index": current_index,
        "total_fields": len(fields),
        "field_name": field["field_name"],
        "question": question_text,
        "voice_url": voice_url,  # Voice note for this field question
        "field_type": field.get("field_type", "text"),
    }


# üîπ SUBMIT FIELD RESPONSE
@app.post("/api/submit-field")
async def submit_field(
    session_id: str = Form(...),
    field_value: str = Form(...)
):
    """Submit user's response for current field and get next question"""
    if session_id not in conversation_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = conversation_sessions[session_id]
    fields = session["form_analysis"].get("fields", [])
    current_index = session["current_field_index"]
    
    if current_index < len(fields):
        field_name = fields[current_index]["field_name"]
        session["field_responses"][field_name] = field_value
        session["current_field_index"] += 1
    
    # Get next question or complete
    return await _get_next_field_question(session_id)


# üîπ SPEECH TO TEXT (for voice input)
@app.post("/api/speech-to-text")
async def speech_to_text(
    audio: UploadFile = File(...),
    language: str = Form(default="en")
):
    """Convert speech audio to text - supports Hindi and English recognition"""
    try:
        # Save audio temporarily
        audio_filename = f"{uuid.uuid4()}.{audio.filename.split('.')[-1]}"
        audio_path = os.path.join(UPLOAD_DIR, audio_filename)
        
        with open(audio_path, "wb") as f:
            f.write(await audio.read())
        
        # Use Google Speech Recognition
        import speech_recognition as sr
        recognizer = sr.Recognizer()
        
        with sr.AudioFile(audio_path) as source:
            audio_data = recognizer.record(source)
        
        # Map language codes to speech recognition
        lang_map = {
            "en": "en-US",
            "hi": "hi-IN",
            "mr": "mr-IN",
            "ta": "ta-IN",
            "te": "te-IN",
            "bn": "bn-IN",
            "gu": "gu-IN",
            "kn": "kn-IN",
            "ml": "ml-IN",
            "pa": "pa-IN",
        }
        
        recognition_lang = lang_map.get(language, "en-US")
        
        # Try to recognize in selected language
        try:
            text = recognizer.recognize_google(audio_data, language=recognition_lang)
        except (sr.UnknownValueError, sr.RequestError):
            # If Hindi is selected and fails, try English as fallback
            if language == "hi":
                try:
                    text = recognizer.recognize_google(audio_data, language="en-US")
                except:
                    raise sr.UnknownValueError()
            else:
                raise
        
        # Clean up audio file
        try:
            os.remove(audio_path)
        except:
            pass
        
        return {"text": text, "success": True, "detected_language": language}
        
    except sr.UnknownValueError:
        raise HTTPException(status_code=400, detail="Could not understand audio")
    except sr.RequestError as e:
        raise HTTPException(status_code=503, detail=f"Speech recognition service error: {str(e)}")
    except Exception as e:
        print(f"Speech to text error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# üîπ GENERATE FILLED FORM
@app.post("/api/generate-filled-form")
async def generate_filled_form(session_id: str = Form(...)):
    """Generate the filled form based on all responses"""
    if session_id not in conversation_sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    session = conversation_sessions[session_id]
    form_analysis = session["form_analysis"]
    form_language = session.get("form_language", "en")
    responses = session["field_responses"]
    language = session["language"]
    
    # Initialize translator
    translator = Translator()
    
    # Create a formatted filled form
    form_name = form_analysis.get("form_name", "Form")
    
    filled_form_text = f"\n{'='*50}\n"
    filled_form_text += f"{form_name.upper()}\n"
    filled_form_text += f"{'='*50}\n\n"
    
    # Add all fields with responses - translate if needed
    for field in form_analysis.get("fields", []):
        field_name = field["field_name"]
        value = responses.get(field_name, "Not provided")
        
        # Translate answer to match form language if different
        if value and value != "Not provided":
            try:
                # Detect the language of the answer
                detected_lang = detect(value)
                
                # If answer language doesn't match form language, translate it
                if detected_lang != form_language:
                    translation = translator.translate(value, dest=form_language)
                    value = translation.text
                    print(f"Translated '{responses.get(field_name)}' from {detected_lang} to {form_language}: {value}")
            except Exception as e:
                print(f"Translation error for field {field_name}: {e}")
                # Keep original value if translation fails
                pass
        
        filled_form_text += f"{field_name}: {value}\n"
    
    filled_form_text += f"\n{'='*50}\n"
    
    # Create completion message
    completion_texts = {
        "en": f"Form filling completed successfully! All {len(responses)} fields have been filled.",
        "hi": f"‡§´‡•â‡§∞‡•ç‡§Æ ‡§≠‡§∞‡§®‡§æ ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§™‡•Ç‡§∞‡§æ ‡§π‡•Å‡§Ü! ‡§∏‡§≠‡•Ä {len(responses)} ‡§´‡•Ä‡§≤‡•ç‡§° ‡§≠‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§π‡•à‡§Ç‡•§",
        "mr": f"‡§´‡•â‡§∞‡•ç‡§Æ ‡§≠‡§∞‡§£‡•á ‡§Ø‡§∂‡§∏‡•ç‡§µ‡•Ä‡§∞‡§ø‡§§‡•ç‡§Ø‡§æ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ù‡§æ‡§≤‡•á! ‡§∏‡§∞‡•ç‡§µ {len(responses)} ‡§´‡•Ä‡§≤‡•ç‡§° ‡§≠‡§∞‡§≤‡•á ‡§ó‡•á‡§≤‡•á ‡§Ü‡§π‡•á‡§§‡•§",
        "ta": f"‡Æ™‡Æü‡Æø‡Æµ‡ÆÆ‡Øç ‡Æ®‡Æø‡Æ∞‡Æ™‡Øç‡Æ™‡ØÅ‡Æ§‡Æ≤‡Øç ‡Æµ‡ØÜ‡Æ±‡Øç‡Æ±‡Æø‡Æï‡Æ∞‡ÆÆ‡Ææ‡Æï ‡ÆÆ‡ØÅ‡Æü‡Æø‡Æ®‡Øç‡Æ§‡Æ§‡ØÅ! ‡ÆÖ‡Æ©‡Øà‡Æ§‡Øç‡Æ§‡ØÅ {len(responses)} ‡Æ™‡ØÅ‡Æ≤‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡ÆÆ‡Øç ‡Æ®‡Æø‡Æ∞‡Æ™‡Øç‡Æ™‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ©.",
        "te": f"‡∞´‡∞æ‡∞∞‡∞Æ‡±ç ‡∞®‡∞ø‡∞Ç‡∞™‡∞°‡∞Ç ‡∞µ‡∞ø‡∞ú‡∞Ø‡∞µ‡∞Ç‡∞§‡∞Ç‡∞ó‡∞æ ‡∞™‡±Ç‡∞∞‡±ç‡∞§‡∞Ø‡∞ø‡∞Ç‡∞¶‡∞ø! ‡∞Ö‡∞®‡±ç‡∞®‡±Ä {len(responses)} ‡∞´‡±Ä‡∞≤‡±ç‡∞°‡±ç‚Äå‡∞≤‡±Å ‡∞®‡∞ø‡∞Ç‡∞™‡∞¨‡∞°‡±ç‡∞°‡∞æ‡∞Ø‡∞ø.",
        "bn": f"‡¶´‡¶∞‡ßç‡¶Æ ‡¶™‡ßÇ‡¶∞‡¶£ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! ‡¶∏‡¶Æ‡¶∏‡ßç‡¶§ {len(responses)} ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞ ‡¶™‡ßÇ‡¶∞‡¶£ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§",
        "gu": f"‡™´‡´ã‡™∞‡´ç‡™Æ ‡™≠‡™∞‡™µ‡™æ‡™®‡´Å‡™Ç ‡™∏‡™´‡™≥‡™§‡™æ‡™™‡´Ç‡™∞‡´ç‡™µ‡™ï ‡™™‡´Ç‡™∞‡´ç‡™£ ‡™•‡™Ø‡´Å‡™Ç! ‡™¨‡™ß‡™æ {len(responses)} ‡™´‡´Ä‡™≤‡´ç‡™° ‡™≠‡™∞‡™µ‡™æ‡™Æ‡™æ‡™Ç ‡™Ü‡™µ‡´ç‡™Ø‡™æ ‡™õ‡´á‡•§",
        "kn": f"‡≤´‡≤æ‡≤∞‡≥ç‡≤Æ‡≥ç ‡≤≠‡≤∞‡≥ç‡≤§‡≤ø ‡≤Ø‡≤∂‡≤∏‡≥ç‡≤µ‡≤ø‡≤Ø‡≤æ‡≤ó‡≤ø ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤Ç‡≤°‡≤ø‡≤¶‡≥Ü! ‡≤é‡≤≤‡≥ç‡≤≤‡≤æ {len(responses)} ‡≤ï‡≥ç‡≤∑‡≥á‡≤§‡≥ç‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤≠‡≤∞‡≥ç‡≤§‡≤ø ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü.",
        "ml": f"‡¥´‡µã‡¥Ç ‡¥™‡µÇ‡¥∞‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µΩ ‡¥µ‡¥ø‡¥ú‡¥Ø‡¥ï‡¥∞‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡¥ø‡¥Ø‡¥æ‡¥Ø‡¥ø! ‡¥é‡¥≤‡µç‡¥≤‡¥æ {len(responses)} ‡¥´‡µÄ‡µΩ‡¥°‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥™‡µÇ‡¥∞‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ö‡µç‡¥ö‡µÅ.",
        "pa": f"‡®´‡®æ‡®∞‡®Æ ‡®≠‡®∞‡®®‡®æ ‡®∏‡®´‡®≤‡®§‡®æ‡®™‡©Ç‡®∞‡®µ‡®ï ‡®™‡©Ç‡®∞‡®æ ‡®π‡©ã‡®á‡®Ü! ‡®∏‡®æ‡®∞‡©á {len(responses)} ‡®´‡©Ä‡®≤‡®° ‡®≠‡®∞ ‡®¶‡®ø‡©±‡®§‡©á ‡®ó‡®è ‡®π‡®®‡•§",
    }
    
    summary_message = completion_texts.get(language, completion_texts["en"])
    voice_url, _ = _create_voice_note(summary_message, language)
    
    return {
        "success": True,
        "summary": summary_message,
        "voice_url": voice_url,
        "field_responses": responses,
        "filled_form_text": filled_form_text,
        "form_language": form_language,
        "form_name": form_name,
        "message": summary_message
    }


# ---------------- RUN ----------------
if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
